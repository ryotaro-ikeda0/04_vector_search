{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 事前準備（ライブラリのインポートなど）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準ライブラリ\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 自然言語系のライブラリ\n",
    "import sudachipy\n",
    "from gensim.summarization.bm25 import BM25\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 形態素分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストップワード（処理対象外の単語）の読み込み\n",
    "!wget -nv -N -P \"../data/\" \"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\"\n",
    "\n",
    "with open(\"../data/Japanese.txt\",\"r\") as f:\n",
    "    stop_words = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neologdによるトーカナイザー(リストで返す関数・名詞のみ)\n",
    "def sudachi_tokenizer(text: str) -> list:\n",
    "\n",
    "    # 正規表現による文章の修正\n",
    "    replaced_text = text.lower() # 全て小文字へ変換\n",
    "    replaced_text = re.sub(r'[【】]', '', replaced_text) # 【】の除去\n",
    "    replaced_text = re.sub(r'[（）()]', '', replaced_text) # （）の除去\n",
    "    replaced_text = re.sub(r'[［］\\[\\]]', '', replaced_text) # ［］の除去\n",
    "    replaced_text = re.sub(r'[@＠]\\w+', '', replaced_text) # メンションの除去\n",
    "    replaced_text = re.sub(r'\\d+\\.*\\d*', '', replaced_text) #数字を0にする\n",
    "    replaced_text = re.sub(r'[*、％]', '', replaced_text) # 記号の除去\n",
    "\n",
    "    # sudachi tokenize\n",
    "    tokenizer_obj = sudachipy.Dictionary().create()\n",
    "    mode = sudachipy.Tokenizer.SplitMode.C\n",
    "\n",
    "    parsed_lines = tokenizer_obj.tokenize(replaced_text, mode)\n",
    "\n",
    "    # 名詞かつ固有名詞ではないものに絞り込み\n",
    "    token_list = [t.surface() for t in parsed_lines if t.part_of_speech()[0] == \"名詞\" and t.part_of_speech()[1] != \"固有名詞\"]\n",
    "\n",
    "    # stop wordsの除去\n",
    "    token_list = [t for t in token_list if t  not in stop_words]\n",
    "\n",
    "    # ひらがなのみの単語を除く\n",
    "    kana_re = re.compile(\"^[ぁ-ゖ]+$\")\n",
    "    token_list = [t for t in token_list if not kana_re.match(t)]\n",
    "\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "クエリ: ガンジス川は汚い\n",
      "\n",
      "・検索結果\n",
      "1.633 ガンジス川を見るためにインドに来た。\n",
      "0.0 インドカレーが好きだ。\n"
     ]
    }
   ],
   "source": [
    "class best_match:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = sudachi_tokenizer\n",
    "\n",
    "    #前処理\n",
    "    def pre_process(self, docs):\n",
    "        self.docs = docs\n",
    "        corpus = [self.wakachi(doc) for doc in self.docs]\n",
    "        self.bm25_ = BM25(corpus)\n",
    "\n",
    "    #クエリとの順位付け\n",
    "    def ranking(self, query):\n",
    "        wakachi_query = self.wakachi(query)\n",
    "        self.scores = self.bm25_.get_scores(wakachi_query)\n",
    "\n",
    "    #分かち書き\n",
    "    def wakachi(self, doc):\n",
    "        return list(self.tokenizer(doc))\n",
    "\n",
    "    #上位n件を抽出\n",
    "    def select_docs(self, num):\n",
    "        docs_dict = dict(zip(self.scores, self.docs))\n",
    "        docs_dict = dict(sorted(docs_dict.items(), reverse = True))\n",
    "        print(\"\\n・検索結果\")\n",
    "        i = 0\n",
    "        for key, value in docs_dict.items():\n",
    "            print(round(key, 3), value)\n",
    "            i += 1\n",
    "            if i == num: break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"インドカレー屋で提供されているラッシーは、とても美味しい。\"\n",
    "    docs = [\"カレーよりもシチューが好きだ。\",\n",
    "            \"ガンジス川を見るためにインドに来た。\",\n",
    "            \"カレーが好きだ。中でも、インドカレーが一番好きだ。\",\n",
    "            \"自宅で作ったラッシーも美味しい。\",\n",
    "            \"欧風カレーとインドカレーは全くの別物だが、どちらも美味しい。\",\n",
    "            \"インドカレーが好きだ。\"]\n",
    "    while True:\n",
    "        try:\n",
    "            num = int(input(\"検索数を自然数で入力してください:\"))\n",
    "            if num <= 0:\n",
    "                print(\"0より大きな数字を入力してください。\")\n",
    "            elif num < len(docs):\n",
    "                break\n",
    "            else:\n",
    "                print(\"文書数より多い数字が入力されています。\")\n",
    "        except Exception:\n",
    "            print(\"数字以外のテキストが入力されています。\")\n",
    "\n",
    "    print(\"クエリ:\", query)\n",
    "    inst_BM = best_match()\n",
    "    inst_BM.pre_process(docs)\n",
    "    inst_BM.ranking(query)\n",
    "    inst_BM.select_docs(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
